{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038dd83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  deprecation(\n",
      "2022-05-24 15:34:22.837034: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-24 15:34:22.837070: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-24 15:34:22.837474: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 0.90 at episode 1\n",
      "running reward: 1.91 at episode 2\n",
      "running reward: 2.91 at episode 3\n",
      "running reward: 3.66 at episode 4\n",
      "running reward: 4.93 at episode 5\n",
      "running reward: 5.53 at episode 6\n",
      "running reward: 5.96 at episode 7\n",
      "running reward: 7.91 at episode 8\n",
      "running reward: 10.61 at episode 9\n",
      "running reward: 11.68 at episode 10\n",
      "running reward: 12.05 at episode 11\n",
      "running reward: 12.45 at episode 12\n",
      "running reward: 16.77 at episode 13\n",
      "running reward: 17.54 at episode 14\n",
      "running reward: 18.56 at episode 15\n",
      "running reward: 19.53 at episode 16\n",
      "running reward: 20.45 at episode 17\n",
      "running reward: 21.33 at episode 18\n",
      "running reward: 23.87 at episode 19\n",
      "running reward: 23.67 at episode 20\n",
      "running reward: 24.29 at episode 21\n",
      "running reward: 26.32 at episode 22\n",
      "running reward: 27.06 at episode 23\n",
      "running reward: 26.45 at episode 24\n",
      "running reward: 26.08 at episode 25\n",
      "running reward: 25.83 at episode 26\n",
      "running reward: 25.79 at episode 27\n",
      "running reward: 26.85 at episode 28\n",
      "running reward: 27.15 at episode 29\n",
      "running reward: 26.95 at episode 30\n",
      "running reward: 28.80 at episode 31\n",
      "running reward: 29.61 at episode 32\n",
      "running reward: 29.03 at episode 33\n",
      "running reward: 30.28 at episode 34\n",
      "running reward: 29.56 at episode 35\n",
      "running reward: 29.29 at episode 36\n",
      "running reward: 28.67 at episode 37\n",
      "running reward: 27.84 at episode 38\n",
      "running reward: 27.10 at episode 39\n",
      "running reward: 26.39 at episode 40\n",
      "running reward: 25.77 at episode 41\n",
      "running reward: 25.58 at episode 42\n",
      "running reward: 24.90 at episode 43\n",
      "running reward: 24.26 at episode 44\n",
      "running reward: 23.65 at episode 45\n",
      "running reward: 22.91 at episode 46\n",
      "running reward: 22.37 at episode 47\n",
      "running reward: 21.65 at episode 48\n",
      "running reward: 21.32 at episode 49\n",
      "running reward: 21.10 at episode 50\n",
      "running reward: 21.10 at episode 51\n",
      "running reward: 20.84 at episode 52\n",
      "running reward: 20.60 at episode 53\n",
      "running reward: 20.57 at episode 54\n",
      "running reward: 20.14 at episode 55\n",
      "running reward: 19.63 at episode 56\n",
      "running reward: 20.25 at episode 57\n",
      "running reward: 20.19 at episode 58\n",
      "running reward: 19.68 at episode 59\n",
      "running reward: 19.45 at episode 60\n",
      "running reward: 19.12 at episode 61\n",
      "running reward: 19.02 at episode 62\n",
      "running reward: 18.57 at episode 63\n",
      "running reward: 18.44 at episode 64\n",
      "running reward: 19.12 at episode 65\n",
      "running reward: 18.71 at episode 66\n",
      "running reward: 19.38 at episode 67\n",
      "running reward: 20.51 at episode 68\n",
      "running reward: 22.28 at episode 69\n",
      "running reward: 22.12 at episode 70\n",
      "running reward: 21.81 at episode 71\n",
      "running reward: 21.32 at episode 72\n",
      "running reward: 20.90 at episode 73\n",
      "running reward: 22.36 at episode 74\n",
      "running reward: 21.99 at episode 75\n",
      "running reward: 22.59 at episode 76\n",
      "running reward: 23.91 at episode 77\n",
      "running reward: 23.67 at episode 78\n",
      "running reward: 24.33 at episode 79\n",
      "running reward: 24.92 at episode 80\n",
      "running reward: 25.02 at episode 81\n",
      "running reward: 24.52 at episode 82\n",
      "running reward: 24.39 at episode 83\n",
      "running reward: 26.32 at episode 84\n",
      "running reward: 25.76 at episode 85\n",
      "running reward: 29.32 at episode 86\n",
      "running reward: 30.00 at episode 87\n",
      "running reward: 32.55 at episode 88\n",
      "running reward: 36.78 at episode 89\n",
      "running reward: 44.94 at episode 90\n",
      "running reward: 51.19 at episode 91\n",
      "running reward: 55.88 at episode 92\n",
      "running reward: 54.99 at episode 93\n",
      "running reward: 56.94 at episode 94\n",
      "running reward: 64.09 at episode 95\n",
      "running reward: 66.09 at episode 96\n",
      "running reward: 67.83 at episode 97\n",
      "running reward: 71.49 at episode 98\n",
      "running reward: 76.87 at episode 99\n",
      "running reward: 83.02 at episode 100\n",
      "running reward: 88.87 at episode 101\n",
      "running reward: 94.43 at episode 102\n",
      "running reward: 99.71 at episode 103\n",
      "running reward: 104.72 at episode 104\n",
      "running reward: 109.48 at episode 105\n",
      "running reward: 114.01 at episode 106\n",
      "running reward: 118.31 at episode 107\n",
      "running reward: 122.39 at episode 108\n",
      "running reward: 126.27 at episode 109\n",
      "running reward: 129.96 at episode 110\n",
      "running reward: 133.46 at episode 111\n",
      "running reward: 136.79 at episode 112\n",
      "running reward: 137.95 at episode 113\n",
      "running reward: 139.55 at episode 114\n",
      "running reward: 142.23 at episode 115\n",
      "running reward: 142.66 at episode 116\n",
      "running reward: 141.88 at episode 117\n",
      "running reward: 143.69 at episode 118\n",
      "running reward: 143.10 at episode 119\n",
      "running reward: 141.55 at episode 120\n",
      "running reward: 140.52 at episode 121\n",
      "running reward: 140.89 at episode 122\n",
      "running reward: 140.45 at episode 123\n",
      "running reward: 139.08 at episode 124\n",
      "running reward: 137.97 at episode 125\n",
      "running reward: 132.27 at episode 126\n",
      "running reward: 133.06 at episode 127\n",
      "running reward: 133.31 at episode 128\n",
      "running reward: 134.04 at episode 129\n",
      "running reward: 137.34 at episode 130\n",
      "running reward: 139.67 at episode 131\n",
      "running reward: 135.49 at episode 132\n",
      "running reward: 138.71 at episode 133\n",
      "running reward: 137.18 at episode 134\n",
      "running reward: 138.37 at episode 135\n",
      "running reward: 141.05 at episode 136\n",
      "running reward: 142.80 at episode 137\n",
      "running reward: 145.66 at episode 138\n",
      "running reward: 145.93 at episode 139\n",
      "running reward: 148.63 at episode 140\n",
      "running reward: 151.20 at episode 141\n",
      "running reward: 153.49 at episode 142\n",
      "running reward: 154.86 at episode 143\n",
      "running reward: 157.12 at episode 144\n",
      "running reward: 159.26 at episode 145\n",
      "running reward: 160.40 at episode 146\n",
      "running reward: 158.18 at episode 147\n",
      "running reward: 156.27 at episode 148\n",
      "running reward: 155.46 at episode 149\n",
      "running reward: 157.69 at episode 150\n",
      "running reward: 159.80 at episode 151\n",
      "running reward: 161.81 at episode 152\n",
      "running reward: 163.72 at episode 153\n",
      "running reward: 164.08 at episode 154\n",
      "running reward: 162.38 at episode 155\n",
      "running reward: 164.26 at episode 156\n",
      "running reward: 166.05 at episode 157\n",
      "running reward: 167.75 at episode 158\n",
      "running reward: 167.61 at episode 159\n",
      "running reward: 169.23 at episode 160\n",
      "running reward: 168.02 at episode 161\n",
      "running reward: 166.42 at episode 162\n",
      "running reward: 165.40 at episode 163\n",
      "running reward: 165.73 at episode 164\n",
      "running reward: 164.24 at episode 165\n",
      "running reward: 162.53 at episode 166\n",
      "running reward: 160.05 at episode 167\n",
      "running reward: 157.60 at episode 168\n",
      "running reward: 157.27 at episode 169\n",
      "running reward: 155.16 at episode 170\n",
      "running reward: 152.45 at episode 171\n",
      "running reward: 150.97 at episode 172\n",
      "running reward: 150.93 at episode 173\n",
      "running reward: 152.63 at episode 174\n",
      "running reward: 153.35 at episode 175\n",
      "running reward: 153.43 at episode 176\n",
      "running reward: 153.26 at episode 177\n",
      "running reward: 154.60 at episode 178\n",
      "running reward: 156.87 at episode 179\n",
      "running reward: 149.87 at episode 180\n",
      "running reward: 151.08 at episode 181\n",
      "running reward: 151.93 at episode 182\n",
      "running reward: 152.08 at episode 183\n",
      "running reward: 152.48 at episode 184\n",
      "running reward: 154.85 at episode 185\n",
      "running reward: 156.76 at episode 186\n",
      "running reward: 158.92 at episode 187\n",
      "running reward: 160.98 at episode 188\n",
      "running reward: 162.93 at episode 189\n",
      "running reward: 164.78 at episode 190\n",
      "running reward: 166.54 at episode 191\n",
      "running reward: 168.21 at episode 192\n",
      "running reward: 169.80 at episode 193\n",
      "running reward: 171.31 at episode 194\n",
      "running reward: 172.75 at episode 195\n",
      "running reward: 174.11 at episode 196\n",
      "running reward: 175.40 at episode 197\n",
      "running reward: 176.63 at episode 198\n",
      "running reward: 177.80 at episode 199\n",
      "running reward: 178.91 at episode 200\n",
      "running reward: 179.97 at episode 201\n",
      "running reward: 180.97 at episode 202\n",
      "running reward: 181.92 at episode 203\n",
      "running reward: 182.82 at episode 204\n",
      "running reward: 183.68 at episode 205\n",
      "running reward: 184.50 at episode 206\n",
      "running reward: 185.27 at episode 207\n",
      "running reward: 186.01 at episode 208\n",
      "running reward: 186.71 at episode 209\n",
      "running reward: 187.37 at episode 210\n",
      "running reward: 188.01 at episode 211\n",
      "running reward: 188.61 at episode 212\n",
      "running reward: 189.17 at episode 213\n",
      "running reward: 189.72 at episode 214\n",
      "running reward: 181.03 at episode 215\n",
      "running reward: 181.98 at episode 216\n",
      "running reward: 182.88 at episode 217\n",
      "running reward: 183.74 at episode 218\n",
      "running reward: 184.55 at episode 219\n",
      "running reward: 179.77 at episode 220\n",
      "running reward: 172.43 at episode 221\n",
      "running reward: 172.86 at episode 222\n",
      "running reward: 174.22 at episode 223\n",
      "running reward: 175.51 at episode 224\n",
      "running reward: 167.73 at episode 225\n",
      "running reward: 169.35 at episode 226\n",
      "running reward: 170.88 at episode 227\n",
      "running reward: 172.33 at episode 228\n",
      "running reward: 173.72 at episode 229\n",
      "running reward: 175.03 at episode 230\n",
      "running reward: 176.28 at episode 231\n",
      "running reward: 177.47 at episode 232\n",
      "running reward: 170.54 at episode 233\n",
      "running reward: 172.02 at episode 234\n",
      "running reward: 173.41 at episode 235\n",
      "running reward: 173.54 at episode 236\n",
      "running reward: 165.47 at episode 237\n",
      "running reward: 160.29 at episode 238\n",
      "running reward: 153.03 at episode 239\n",
      "running reward: 148.43 at episode 240\n",
      "running reward: 151.01 at episode 241\n",
      "running reward: 144.31 at episode 242\n",
      "running reward: 144.09 at episode 243\n",
      "running reward: 144.84 at episode 244\n",
      "running reward: 145.84 at episode 245\n",
      "running reward: 140.40 at episode 246\n",
      "running reward: 143.38 at episode 247\n",
      "running reward: 146.21 at episode 248\n",
      "running reward: 147.35 at episode 249\n",
      "running reward: 149.98 at episode 250\n",
      "running reward: 152.49 at episode 251\n",
      "running reward: 154.86 at episode 252\n",
      "running reward: 157.12 at episode 253\n",
      "running reward: 159.26 at episode 254\n",
      "running reward: 161.30 at episode 255\n",
      "running reward: 163.23 at episode 256\n",
      "running reward: 165.07 at episode 257\n",
      "running reward: 166.82 at episode 258\n",
      "running reward: 168.48 at episode 259\n",
      "running reward: 170.05 at episode 260\n",
      "running reward: 171.55 at episode 261\n",
      "running reward: 172.97 at episode 262\n",
      "running reward: 174.32 at episode 263\n",
      "running reward: 175.61 at episode 264\n",
      "running reward: 176.83 at episode 265\n",
      "running reward: 177.99 at episode 266\n",
      "running reward: 179.09 at episode 267\n",
      "running reward: 180.13 at episode 268\n",
      "running reward: 181.13 at episode 269\n",
      "running reward: 182.07 at episode 270\n",
      "running reward: 182.97 at episode 271\n",
      "running reward: 183.82 at episode 272\n",
      "running reward: 184.63 at episode 273\n",
      "running reward: 185.40 at episode 274\n",
      "running reward: 186.13 at episode 275\n",
      "running reward: 186.82 at episode 276\n",
      "running reward: 187.48 at episode 277\n",
      "running reward: 188.10 at episode 278\n",
      "running reward: 188.70 at episode 279\n",
      "running reward: 189.26 at episode 280\n",
      "running reward: 189.80 at episode 281\n",
      "running reward: 190.31 at episode 282\n",
      "running reward: 190.80 at episode 283\n",
      "running reward: 191.26 at episode 284\n",
      "running reward: 191.69 at episode 285\n",
      "running reward: 192.11 at episode 286\n",
      "running reward: 192.50 at episode 287\n",
      "running reward: 192.88 at episode 288\n",
      "running reward: 193.23 at episode 289\n",
      "running reward: 193.57 at episode 290\n",
      "running reward: 193.89 at episode 291\n",
      "running reward: 194.20 at episode 292\n",
      "running reward: 194.49 at episode 293\n",
      "running reward: 194.76 at episode 294\n",
      "running reward: 195.03 at episode 295\n",
      "Solved at episode 295!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
    "env.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n",
    "\n",
    "num_inputs = 4\n",
    "num_actions = 2\n",
    "num_hidden = 128\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.007)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # env.render(); Adding this line would show the attempts\n",
    "            # of the agent in a pop up window.\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    template = \"running reward: {:.2f} at episode {}\"\n",
    "    print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 195:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a95c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7ebb5c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
