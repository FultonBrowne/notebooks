{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday 3.1\n",
    "The ideas of Friday 3, but maybe it will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract:\n",
    "Friday is a digital system designed to be a starting point for a truewly useful, JARVIS level, assistant. This will combine the lastest of Genreative learning, input proccessing, and self supervised learning.\n",
    "\n",
    "The hope of friday is that a minimal model can branch out using the internet to master specific skills and while activly learning from mistakes and successes.\n",
    "\n",
    "This will all be done on a single, interconnected model written in torch.\n",
    "\n",
    "Let's get going...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First thing we need to do is import all the neccesary modules. We want to use primarly large, established projects, but if we need code from a small one we'll use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import wikipediaapi\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "Just some variables that need to be defined and not changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters\n",
    "Tunable paramters that allow you to change the way friday operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilty's\n",
    "The boring stuff, logging and other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: test\n"
     ]
    }
   ],
   "source": [
    "def log(area, message):\n",
    "    print(\"%s:\" % area, message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Internet Brain\n",
    "The tools that will link a brain to the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My little Wikipedia utilty I use sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiBrain:\n",
    "    def __init__(self, lang):\n",
    "        self.wiki = wikipediaapi.Wikipedia(lang)\n",
    "\n",
    "    def pageexist(self, term):\n",
    "        return self.wiki.page(term)\n",
    "    \n",
    "    def summary(self, page):\n",
    "        return self.wiki.page(page).summary\n",
    "    def full_page(self, page):\n",
    "        return self.wiki.page(page).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = WikiBrain(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask function\n",
    "Super basic now, but will expand it as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, second_pass:bool = False):\n",
    "    if(second_pass): #If more detailed data is needed - most likly if the first pass of this function didn't work\n",
    "        return wb.full_page(query)\n",
    "    return wb.summary(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data model\n",
    "\n",
    "Load up the model and fine tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\") # use this for now, will probably fine tune GPT-j ot t0pp\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "The brain itslef, fridays core innovation will be right here. \n",
    "\n",
    "Friday's core algrothim is a hybrid supervised/unsupervised learning model that can ideally learn as it goes and be told when it's wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now it's just a simple chat bot - we'll need to add on the fly learning at some point (using ask() to get the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "while True:\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    convinput = input()\n",
    "    if(convinput == \"stop\"):\n",
    "        break\n",
    "    new_user_input_ids = tokenizer.encode(convinput + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response \n",
    "    chat_history_ids = model.generate(bot_input_ids, pad_token_id=tokenizer.eos_token_id)\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
