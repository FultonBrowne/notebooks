{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday 3.1\n",
    "The ideas of Friday 3, but maybe it will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract:\n",
    "Friday is a digital system designed to be a starting point for a truewly useful, JARVIS level, assistant. This will combine the lastest of Genreative learning, input proccessing, and self supervised learning.\n",
    "\n",
    "The hope of friday is that a minimal model can branch out using the internet to master specific skills and while activly learning from mistakes and successes.\n",
    "\n",
    "This will all be done on a single, interconnected model written in torch.\n",
    "\n",
    "Let's get going...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First thing we need to do is import all the neccesary modules. We want to use primarly large, established projects, but if we need code from a small one we'll use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import exists\n",
    "import wikipediaapi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,  AutoModelForSeq2SeqLM, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "Just some variables that need to be defined and not changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query types\n",
    "QUERY_TYPE_GENERAL = 0\n",
    "QUERY_TYPE_DEFINITION = 1\n",
    "QUERY_TYPE_DETAILS = 2\n",
    "QUERY_TYPE_EXAMPLES = 3\n",
    "\n",
    "# Data retrieval types\n",
    "# Will be used for retrieving specific data by title\n",
    "DATA_TYPE_BOOK = 0\n",
    "DATA_TYPE_ARTICLE = 1\n",
    "DATA_TYPE_WEBPAGE = 2\n",
    "DATA_TYPE_CODE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters\n",
    "Tunable paramters that allow you to change the way friday operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Friday\"\n",
    "PROMPT = \"Hello, I'm Friday. What can I do for you?\"\n",
    "FRIDAY_LIBRARY = \"./friday.cvs\" # A refrence database built solely by the model for data storage - fridays notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilty's\n",
    "The boring stuff, logging and other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(area, message):\n",
    "    print(\"%s:\" % area, message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Internet Brain\n",
    "The tools that will link a brain to the internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format the ask() function will return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My little Wikipedia utilty I use sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiBrain:\n",
    "    def __init__(self, lang):\n",
    "        self.wiki = wikipediaapi.Wikipedia(lang)\n",
    "\n",
    "    def pageexist(self, term):\n",
    "        return self.wiki.page(term)\n",
    "    \n",
    "    def summary(self, page):\n",
    "        return self.wiki.page(page).summary\n",
    "    def full_page(self, page):\n",
    "        return self.wiki.page(page).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = WikiBrain(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask function\n",
    "Super basic now, but will expand it as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, second_pass:bool = False):\n",
    "    if(second_pass): #If more detailed data is needed - most likly if the first pass of this function didn't work\n",
    "        return wb.full_page(query)\n",
    "    return wb.summary(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action function\n",
    "This will be used to let Friday interact with the outside world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact storage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fl():\n",
    "    if(exists(FRIDAY_LIBRARY)):\n",
    "        return pd.read_csv(FRIDAY_LIBRARY)\n",
    "    # Create the dataframe\n",
    "    return pd.DataFrame({\"fact\": pd.Series(), \"context\": pd.Series}) # Not the final form "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday layers\n",
    "The extra layer that makes friday friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class friday_layer (nn.Module):\n",
    "\n",
    "    def __init__(self, model): # Could be gpt or t0pp idk yet\n",
    "        self.model = model\n",
    "        # Do stuff lol\n",
    "    # kinda ripping https://github.com/Himabindugssn/Sentiment-classification-using-transformers/blob/main/BERTFinetuning.ipynb\n",
    "    def forward(self, sent_id, mask):\n",
    "        _, cls_hs = self.model(sent_id, attention_mask=mask, return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data model\n",
    "\n",
    "Load up the model and fine tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.1.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.1.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m#tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.1.ipynb#ch0000003vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m#model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.1.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/DialoGPT-large\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# use this for now, will probably fine tune GPT-j or t0pp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.1.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39mAutoModelWithLMHead\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/DialoGPT-large\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\") # use this for now, will probably fine tune GPT-j or t0pp\n",
    "model =AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fine tune on my thursday dataset.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now it's just a simple chat bot - we'll need to add on the fly learning at some point (using ask() to get the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "while True:\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    convinput = input()\n",
    "    if(convinput == \"stop\"):\n",
    "        break\n",
    "    new_user_input_ids = tokenizer.encode(convinput + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
