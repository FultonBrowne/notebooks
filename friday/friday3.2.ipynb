{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday 3.2\n",
    "let's try this again but rip off ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 16:02:43.267586: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-04 16:02:43.267616: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,  AutoModelForSeq2SeqLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model and tokenizer names\n",
    "model_name = \"bigscience/T0_3B\"\n",
    "tokenizer_name = \"bigscience/T0_3B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model =  AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level fine tunning\n",
    "Do a statndard fine tune on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The finetune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_dataloader, val_dataloader, epochs=2):\n",
    "  # Set up the trainer\n",
    "  training_args = TrainingArguments(\n",
    "      output_dir=\"./results\",          # output directory\n",
    "      num_train_epochs=epochs,         # number of epochs\n",
    "      per_device_train_batch_size=16,  # batch size\n",
    "      per_device_eval_batch_size=64,   # evaluation batch size\n",
    "      warmup_steps=500,                # number of warmup steps\n",
    "      weight_decay=0.01,               # L2 weight decay\n",
    "      logging_dir=\"./logs\"             # logging directory\n",
    "  )\n",
    "\n",
    "  trainer = Trainer(\n",
    "      model=model,                     # the model to train\n",
    "      args=training_args,              # training arguments\n",
    "      train_dataset=train_dataloader,  # training dataset\n",
    "      eval_dataset=val_dataloader      # validation dataset\n",
    "  )\n",
    "\n",
    "  # Fine-tune the model\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Anthropic--hh-rlhf-cd8c0320f46686b1\n",
      "Reusing dataset json (/home/vscode/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-cd8c0320f46686b1/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8bbf53bb6c42e7870e1c389c27414b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7fa03b1957e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb7033abe534c3c83d751bd578f5cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a505d58a0b54a008eb90c70d6618d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"chosen\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "val_dataloader = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: chosen, rejected. If chosen, rejected are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 160800\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20100\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "fine_tune(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2740505/335509339.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(tokenizer.encode(' '.join(chat_history) + tokenizer.eos_token + prompt, return_tensors='pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a student at the University of Southern California.\n",
      "There is a problem with the server.\n",
      "There is a problem with the server.\n",
      "There is a problem with the server.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.2.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=32'>33</a>\u001b[0m     \u001b[39m# Add the chatbot's response to the chat history\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=33'>34</a>\u001b[0m     chat_history\u001b[39m.\u001b[39mappend(response)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=34'>35</a>\u001b[0m chatbot(\u001b[39m\"\u001b[39;49m\u001b[39myou>>\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.2.ipynb Cell 14'\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=17'>18</a>\u001b[0m chat_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=19'>20</a>\u001b[0m   \u001b[39m# Get the user's input\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=20'>21</a>\u001b[0m   user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=21'>22</a>\u001b[0m   \u001b[39mif\u001b[39;00m user_input\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000002vscode-remote?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGoodbye!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, chat_history=[]):\n",
    "  # Encode the prompt and generate a response\n",
    "  # input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\n",
    "  input_ids = torch.tensor(tokenizer.encode(' '.join(chat_history) + tokenizer.eos_token + prompt, return_tensors='pt'))\n",
    "  response = model.generate(input_ids, max_length=1024, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  # Decode the response and return it\n",
    "  return tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the chatbot with a prompt\n",
    "prompt = \"Hello there, how are you?\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n",
    "\n",
    "\n",
    "def chatbot(prompt):\n",
    "  \"\"\"Chatbot main loop. Prompt the user for input and generate a response.\"\"\"\n",
    "  chat_history = []\n",
    "  while True:\n",
    "    # Get the user's input\n",
    "    user_input = input(prompt)\n",
    "    if user_input.lower() == 'exit':\n",
    "      print(\"Goodbye!\")\n",
    "      break\n",
    "\n",
    "    # Add the user's input to the chat history\n",
    "    chat_history.append(user_input)\n",
    "\n",
    "    # Generate a response\n",
    "    response = generate_response(user_input, chat_history)\n",
    "    print(response)\n",
    "\n",
    "    # Add the chatbot's response to the chat history\n",
    "    chat_history.append(response)\n",
    "chatbot(\"you>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friday: \n",
      "Friday: \n",
      "Friday: \n",
      "Friday: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.2.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=13'>14</a>\u001b[0m     step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=15'>16</a>\u001b[0m \u001b[39m# Run the chatbot\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=16'>17</a>\u001b[0m chatbot()\n",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.2.ipynb Cell 15'\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m bot_input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([chat_history_ids, new_user_input_ids], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m step \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m new_user_input_ids\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m \u001b[39m# generated a response while limiting the total chat history to 1000 tokens, \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=9'>10</a>\u001b[0m chat_history_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(bot_input_ids, max_length\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m# pretty print last ouput tokens from bot\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFriday: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(tokenizer\u001b[39m.\u001b[39mdecode(chat_history_ids[:, bot_input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:][\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation_utils.py:1181\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[1;32m   1175\u001b[0m         inputs_tensor, pad_token_id, eos_token_id\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1179\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1182\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1183\u001b[0m     )\n\u001b[1;32m   1185\u001b[0m \u001b[39m# 4. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation_utils.py:525\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    523\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    524\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 525\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    527\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1033\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1021\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1022\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m     )\n\u001b[1;32m   1032\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1034\u001b[0m         hidden_states,\n\u001b[1;32m   1035\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1036\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1037\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1038\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1039\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1040\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1041\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1042\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1043\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1044\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1045\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:716\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    713\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    715\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    718\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:326\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    325\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 326\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    327\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:305\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 305\u001b[0m     hidden_gelu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwi_0(hidden_states))\n\u001b[1;32m    306\u001b[0m     hidden_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi_1(hidden_states)\n\u001b[1;32m    307\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_gelu \u001b[39m*\u001b[39m hidden_linear\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/activations.py:34\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(math\u001b[39m.\u001b[39;49msqrt(\u001b[39m2.0\u001b[39;49m \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49mpi) \u001b[39m*\u001b[39m (\u001b[39minput\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39minput\u001b[39m, \u001b[39m3.0\u001b[39m))))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "  step = 0\n",
    "  chat_history = []\n",
    "  while(True):\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"Friday: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "    step = step + 1\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
