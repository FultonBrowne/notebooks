{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday 3.2\n",
    "let's try this again but rip off ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model and tokenizer names\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer_name = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level fine tunning\n",
    "Do a statndard fine tune on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The finetune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, train_dataloader, val_dataloader, epochs=2):\n",
    "  # Set up the trainer\n",
    "  training_args = TrainingArguments(\n",
    "      output_dir=\"./results\",          # output directory\n",
    "      num_train_epochs=epochs,         # number of epochs\n",
    "      per_device_train_batch_size=16,  # batch size\n",
    "      per_device_eval_batch_size=64,   # evaluation batch size\n",
    "      warmup_steps=500,                # number of warmup steps\n",
    "      weight_decay=0.01,               # L2 weight decay\n",
    "      logging_dir=\"./logs\"             # logging directory\n",
    "  )\n",
    "\n",
    "  trainer = Trainer(\n",
    "      model=model,                     # the model to train\n",
    "      args=training_args,              # training arguments\n",
    "      train_dataset=train_dataloader,  # training dataset\n",
    "      eval_dataset=val_dataloader      # validation dataset\n",
    "  )\n",
    "\n",
    "  # Fine-tune the model\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and split it into training and validation sets\n",
    "## VERY TODO\n",
    "train_data, val_data = load_data()\n",
    "\n",
    "# Create dataloaders for the training and validation sets\n",
    "train_dataloader = create_dataloader(train_data)\n",
    "val_dataloader = create_dataloader(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "  # Encode the prompt and generate a response\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "  response = model.generate(input_ids, max_length=1024, top_p=0.9, top_k=20)\n",
    "\n",
    "  # Decode the response and return it\n",
    "  return tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the chatbot with a prompt\n",
    "# prompt = \"Hello there\"\n",
    "# response = generate_response(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the chatbot! Type 'exit' to leave or 'help' for more options.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPTNeoForCausalLM' object has no attribute 'generate_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=28'>29</a>\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mChatbot: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=30'>31</a>\u001b[0m \u001b[39m# Run the chatbot\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=31'>32</a>\u001b[0m chatbot()\n",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.2.ipynb Cell 6'\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=23'>24</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mChatbot: You can type \u001b[39m\u001b[39m'\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to leave the chatbot or ask me a question.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=25'>26</a>\u001b[0m \u001b[39m# Generate response\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=26'>27</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=27'>28</a>\u001b[0m   response \u001b[39m=\u001b[39m generate_response(prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=28'>29</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mChatbot: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/workspaces/ai_tinkering/friday/friday3.2.ipynb Cell 6'\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_response\u001b[39m(prompt):\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m   \u001b[39m# Encode the prompt and generate a response\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m   input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m   response \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate_text(input_ids, max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, top_p\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, top_k\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m   \u001b[39m# Decode the response and return it\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B2f686f6d652f66756c746f6e2f636f64652f61695f74696e6b6572696e67/workspaces/ai_tinkering/friday/friday3.2.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(response[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPTNeoForCausalLM' object has no attribute 'generate_text'"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "  # Welcome message\n",
    "  print(\"Welcome to the chatbot! Type 'exit' to leave or 'help' for more options.\")\n",
    "\n",
    "  while True:\n",
    "    # Get user input\n",
    "    prompt = input(\"You: \")\n",
    "\n",
    "    # Exit the chatbot\n",
    "    if re.match(r\"exit\", prompt, re.I):\n",
    "      print(\"Chatbot: Goodbye!\")\n",
    "      break\n",
    "\n",
    "    # Show help message\n",
    "    elif re.match(r\"help\", prompt, re.I):\n",
    "      print(\"Chatbot: You can type 'exit' to leave the chatbot or ask me a question.\")\n",
    "\n",
    "    # Generate response\n",
    "    else:\n",
    "      response = generate_response(prompt)\n",
    "      print(f\"Chatbot: {response}\")\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
